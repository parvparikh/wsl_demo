[{"id": "r0", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "['At this point, you might be reasonably wondering how far we can really get with linear regression.\\n', 'The world is a complicated place, and we can\u2019t expect linear models to capture the wide variety of\\n', 'functions that we might need to approximate in supervised learning.", "resource_volume": 0, "polyline": {"0": 6, "1": 2, "2": 4, "3": 5, "4": 6, "5": 6}, "ld": {"x": 18.409730170801513, "y": 10.590269829178148}}, {"id": "r1", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "Consider the problem of learning from examples an\\n', 'unknown Boolean function f : {0, 1}D \u2192 {0, 1}, i.e., a function from a binary vector to a single\\n', 'binary value.\nOur goal is to learn a deterministic Boolean formula for this unknown function,\\n', 'using N examples {xn, yn}N\\n', 'n=1 where xn \u2208 {0, 1}D and yn \u2208 {0, 1}.", "resource_volume": 0, "polyline": {"0": 4, "1": 4, "2": 4, "3": 4, "4": 5, "5": 6}, "ld": {"x": 10.679271860441386, "y": 16.320728139516653}}, {"id": "r2", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "We are heading towards the interesting concept of reinforcement\\n', 'learning (RL), in which an agent learns to act in an uncertain environment by training on data that\\n', 'are sequences of state, action, reward, state, action, reward, etc.", "resource_volume": 0, "polyline": {"0": 6, "1": 4, "2": 4, "3": 4, "4": 6, "5": 6}, "ld": {"x": 11.441312758687095, "y": 18.55868724129586}}, {"id": "r3", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "The class of unsupervised\\n', 'learning techniques called dimensionality reduction provides one strategy for such visualization.\\n', 'The central idea is to \ufb01nd an alternative representation of the data that is, say, two dimensional.\nOur\\n', 'brains may have di\ufb03cult time with 1000-dimensional data, but they are well-suited to reasoning\\n', 'about two or three dimensional spaces.", "resource_volume": 0, "polyline": {"0": 4, "1": 3, "2": 4, "3": 6, "4": 4, "5": 6}, "ld": {"x": 17.049517824247946, "y": 9.950482175714077}}, {"id": "r4", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "Finally we will look\\n', 'at some applications.\\n', '\\n', '1 Reinforcement Learning\\n', 'Recall our formulation of the agent\u2019s interaction with the environment.\nThe agent receives a percept,\\n', 'which tells it something about the state of the environment.\nThe agent then takes an action, which\\n']", "resource_volume": 0, "polyline": {"0": 5, "1": 4, "2": 4, "3": 5, "4": 6, "5": 5}, "ld": {"x": 11.67315646921974, "y": 17.326843530759923}}, {"id": "r5", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "['The key insight of neural networks for machine learning is that one can construct powerful\\n', '(e\ufb00ectively nonparametric) function approximators via the composition of di\ufb00erentiable functions.\\n', 'The backpropagation algorithm is a way to compute the gradients needed to \ufb01t the parameters of\\n', 'a neural network, in much the same way we have used gradients for other optimization problems.\\n', 'Backpropagation is a special case of an extraordinarily powerful programming abstraction called\\n', 'automatic di\ufb00erentiation (AD).", "resource_volume": 0, "polyline": {"0": 6, "1": 3, "2": 6, "3": 5, "4": 7, "5": 6}, "ld": {"x": 15.356918409356815, "y": 17.643081590587954}}, {"id": "r6", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "['\u03b1\\n', '\\n', 'N\\U000f3557n=1\\n', '\\n', 's.t.\\n', '\\n', 'yn\u03b1n = 0 and \u03b1n \u2265 0\u2200n \u2208 1, .\n, N .\\n']", "resource_volume": 0, "polyline": {"0": 4, "1": 2, "2": 4, "3": 4, "4": 4, "5": 5}, "ld": {"x": 10.208618409240378, "y": 12.791381590788902}}, {"id": "r7", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "Finally we will look\\n', 'at some applications.\\n', '\\n', '1 Reinforcement Learning\\n', 'Recall our formulation of the agent\u2019s interaction with the environment.\nThe agent receives a percept,\\n', 'which tells it something about the state of the environment.\nThe agent then takes an action, which\\n']", "resource_volume": 0, "polyline": {"0": 5, "1": 4, "2": 4, "3": 5, "4": 6, "5": 5}, "ld": {"x": 11.67315646921974, "y": 17.326843530759923}}, {"id": "r8", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "In classi\ufb01cation, our hypotheses take features in some input space X and map them\\n', 'into a categorical label space like Y = {cat, dog, cow, .\nWe will focus here on binary classi\ufb01cation, in\\n', 'which there are only two output categories and so we can treat Y as {0, 1} or {\u22121, +1} without loss\\n']", "resource_volume": 0, "polyline": {"0": 5, "1": 4, "2": 5, "3": 4, "4": 6, "5": 6}, "ld": {"x": 11.12009627497226, "y": 18.879903725010696}}, {"id": "r9", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "['Linear regression is one of the simplest and most fundamental modeling ideas in statistics and\\n', 'many people would argue that it isn\u2019t even machine learning.\nHowever, linear regression is an\\n', 'excellent starting point for thinking about supervised learning and many of the more sophisticated\\n', 'learning techniques in this course will build upon it in one way or another.", "resource_volume": 0, "polyline": {"0": 6, "1": 3, "2": 5, "3": 5, "4": 6, "5": 6}, "ld": {"x": 13.558855860663092, "y": 17.441144139318666}}, {"id": "r10", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "['In least squares regression, we presented the common viewpoint that our approach to supervised\\n', 'learning be framed in terms of a loss function that scores our predictions relative to the ground\\n', 'truth as determined by the training data.", "resource_volume": 0, "polyline": {"0": 6, "1": 3, "2": 5, "3": 5, "4": 6, "5": 5}, "ld": {"x": 11.441987583232715, "y": 18.55801241675024}}, {"id": "r11", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "['In its broadest de\ufb01nition, machine learning is about automatically discovering structure in data.\\n', 'Those data can take many forms, depending on what our goals are.\nHere we will\\n', 'examine one of the simplest ideas for discovering structure: we will look for groups, or clusters\\n', 'among the data.", "resource_volume": 0, "polyline": {"0": 5, "1": 3, "2": 5, "3": 5, "4": 5, "5": 6}, "ld": {"x": 12.241384390482052, "y": 16.75861560949761}}, {"id": "r12", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "", "resource_volume": 0, "polyline": {"0": 5, "1": 3, "2": 4, "3": 4, "4": 5, "5": 5}, "ld": {"x": 9.039732913580181, "y": 16.960267086418792}}, {"id": "r13", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "This\\n', 'is true even within a restricted class of models such as polynomials of small degree.\nThe design\\n', 'space of machine learning algorithms is huge.\nWhether we\u2019re talking about norm-based penalties for\\n', 'regression models, architectures for deep neural networks, or kernels for support vector machines,\\n', 'we have many decisions to make if we hope to get successful predictions.", "resource_volume": 0, "polyline": {"0": 5, "1": 3, "2": 5, "3": 5, "4": 5, "5": 6}, "ld": {"x": 12.241384390482052, "y": 16.75861560949761}}, {"id": "r14", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "['In least squares regression, we presented the common viewpoint that our approach to supervised\\n', 'learning be framed in terms of a loss function that scores our predictions relative to the ground\\n', 'truth as determined by the training data.", "resource_volume": 0, "polyline": {"0": 6, "1": 3, "2": 5, "3": 5, "4": 6, "5": 5}, "ld": {"x": 11.441987583232715, "y": 18.55801241675024}}, {"id": "r15", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "['When discussing linear regression, we examined two di\ufb00erent points of view that often led\\n', 'to similar algorithms: one based on constructing and minimizing a loss function, and the other\\n', 'based on maximizing the likelihood.", "resource_volume": 0, "polyline": {"0": 6, "1": 3, "2": 4, "3": 5, "4": 6, "5": 5}, "ld": {"x": 12.264060924066262, "y": 16.7359390759134}}, {"id": "r16", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "It takes a very di\ufb00erent approach to binary classi\ufb01cation than does logistic regression.\\n', 'Rather than directly specifying a simple loss function or constructing a likelihood function, the\\n', 'SVM starts with the premise that the data are linearly separable and then identi\ufb01es an intuitive and\\n', 'appealing way to choose among possible separating hyperplanes: maximizing the distance between\\n', 'the \u201chardest\u201d examples and the decision boundary.", "resource_volume": 0, "polyline": {"0": 5, "1": 3, "2": 4, "3": 4, "4": 5, "5": 6}, "ld": {"x": 11.010835410121278, "y": 15.98916458983501}}, {"id": "r17", "name": "name", "type": "resource", "resource_type": "pdf", "resource_summary": "['K-Means clustering is a good general-purpose way to think about discovering groups in data,\\n', 'but there are several aspects of it that are unsatisfying.\nFor another, the\\n', 'notion of what forms a group is very simple: a datum belongs to cluster k if it is closer to the kth\\n', 'center than it is to any other center.", "resource_volume": 0, "polyline": {"0": 5, "1": 3, "2": 6, "3": 4, "4": 4, "5": 5}, "ld": {"x": 7.361360927515468, "y": 19.638639072458957}}]